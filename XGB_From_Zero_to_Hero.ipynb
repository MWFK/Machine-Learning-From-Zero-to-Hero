{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGB From Zero to Hero.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOsSg/ct2gPRKAoy8zKl4o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MWFK/Machine-Learning-From-Zero-to-Hero/blob/main/XGB_From_Zero_to_Hero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOLYP45ESiYw"
      },
      "source": [
        "# Extreme Gradient Boosting Algorithm\r\n",
        "\r\n",
        "Gradient boosting refers to a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems.\r\n",
        "\r\n",
        "Ensembles are constructed from decision tree models. Trees are added one at a time to the ensemble and fit to correct the prediction errors made by prior models. This is a type of ensemble machine learning model referred to as boosting.\r\n",
        "\r\n",
        "Models are fit using any arbitrary differentiable loss function and gradient descent optimization algorithm. This gives the technique its name, “gradient boosting,” as the loss gradient is minimized as the model is fit, much like a neural network.\r\n",
        "\r\n",
        "Extreme Gradient Boosting, or XGBoost for short is an efficient open-source implementation of the gradient boosting algorithm. As such, XGBoost is an algorithm, an open-source project, and a Python library.\r\n",
        "\r\n",
        "It was initially developed by Tianqi Chen and was described by Chen and Carlos Guestrin in their 2016 paper titled “XGBoost: A Scalable Tree Boosting System.”\r\n",
        "\r\n",
        "It is designed to be both computationally efficient (e.g. fast to execute) and highly effective, perhaps more effective than other open-source implementations.\r\n",
        "\r\n",
        "https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/\r\n",
        "\r\n",
        "XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems. The evidence is that it is the go-to algorithm for competition winners on the Kaggle competitive data science platform.\r\n",
        "\r\n",
        "\r\n",
        "With link you will discover how you can estimate the importance of features for a predictive modeling problem using the XGBoost library in Python.\r\n",
        "\r\n",
        "https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/\r\n",
        "\r\n",
        "XGB has a lot of usefull functions you can descover them here\r\n",
        "\r\n",
        "https://xgboost.readthedocs.io/en/latest/parameter.html\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdNx3QSETyvf"
      },
      "source": [
        "# XGBoost Ensemble for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_YB5_gpV5nR"
      },
      "source": [
        "### Create random data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi9MFzV4V-JH"
      },
      "source": [
        "from sklearn.datasets import make_classification\r\n",
        "\r\n",
        "from numpy import mean\r\n",
        "from numpy import std\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\r\n",
        "from xgboost import XGBClassifier\r\n",
        "\r\n",
        "from numpy import asarray\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from xgboost import XGBClassifier\r\n",
        "\r\n",
        "from sklearn.model_selection import RepeatedKFold\r\n",
        "from xgboost import XGBRegressor\r\n",
        "from sklearn.datasets import make_regression"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIcfQ3QcVsP5"
      },
      "source": [
        "# Evaluate XGBoost algorithm for classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrfwF4oJnytU"
      },
      "source": [
        "### Create data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRN_tAB3nx67",
        "outputId": "472f239a-fe45-4070-9305-4dcf389796c9"
      },
      "source": [
        "# define dataset\r\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\r\n",
        "# summarize the dataset\r\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 20) (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2V594Ndn_-E"
      },
      "source": [
        "### Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dba3oFO6U0qX",
        "outputId": "bd8f9d81-be40-4268-8934-a076aaeaf52c"
      },
      "source": [
        "# define dataset\r\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\r\n",
        "# define the model\r\n",
        "model = XGBClassifier()\r\n",
        "# evaluate the model\r\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\r\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\r\n",
        "# report performance\r\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.899 (0.029)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OND35FZld4kU"
      },
      "source": [
        "### Classification Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sywE1HtedW4n",
        "outputId": "43999a9c-6edc-40e0-debb-5d3d3da52d31"
      },
      "source": [
        "model.fit(X, y)\r\n",
        "# make a single prediction\r\n",
        "row = [0.2929949,-4.21223056,-1.288332,-2.17849815,-0.64527665,2.58097719,0.28422388,-7.1827928,-1.91211104,2.73729512,0.81395695,3.96973717,-2.66939799,3.34692332,4.19791821,0.99990998,-0.30201875,-4.43170633,-2.82646737,0.44916808]\r\n",
        "row = asarray([row])\r\n",
        "yhat = model.predict(row)\r\n",
        "print('Predicted Class: %d' % yhat[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Class: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6L0H2Jol4ct"
      },
      "source": [
        "# XGBoost Ensemble for Regression\r\n",
        "\r\n",
        "### Create data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c703Q8M7lwM0",
        "outputId": "6f2a1ab8-dcdf-48de-baca-8013ed24a071"
      },
      "source": [
        "# test regression dataset\r\n",
        "from sklearn.datasets import make_regression\r\n",
        "# define dataset\r\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\r\n",
        "# summarize the dataset\r\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 20) (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5225pP6onvC"
      },
      "source": [
        "### Make model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Riw4FcA1l-2C",
        "outputId": "49e688e8-813c-4872-8487-161e91f3262d"
      },
      "source": [
        "# define dataset\r\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\r\n",
        "# define the model\r\n",
        "model = XGBRegressor()\r\n",
        "# evaluate the model\r\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\r\n",
        "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\r\n",
        "# report performance\r\n",
        "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE: -62.762 (3.219)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRAFic9-mTtu"
      },
      "source": [
        "###  Regression Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-nm-MbFmVd1",
        "outputId": "d61f8edc-f6ef-4b88-e433-3d3a0bb664d2"
      },
      "source": [
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\r\n",
        "# define the model\r\n",
        "model = XGBRegressor()\r\n",
        "# fit the model on the whole dataset\r\n",
        "model.fit(X, y)\r\n",
        "# make a single prediction\r\n",
        "row = [0.20543991,-0.97049844,-0.81403429,-0.23842689,-0.60704084,-0.48541492,0.53113006,2.01834338,-0.90745243,-1.85859731,-1.02334791,-0.6877744,0.60984819,-0.70630121,-1.29161497,1.32385441,1.42150747,1.26567231,2.56569098,-0.11154792]\r\n",
        "row = asarray([row])\r\n",
        "yhat = model.predict(row)\r\n",
        "print('Prediction: %d' % yhat[0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[17:33:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 28\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}