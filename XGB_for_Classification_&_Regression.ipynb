{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGB for Classification & Regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKyr71UGhtGe1qN3hw8d6G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MWFK/Machine-Learning-From-Zero-to-Hero/blob/main/XGB_for_Classification_%26_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOLYP45ESiYw"
      },
      "source": [
        "# Extreme Gradient Boosting Algorithm\n",
        "\n",
        "Gradient boosting refers to a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems.\n",
        "\n",
        "Ensembles are constructed from decision tree models. Trees are added one at a time to the ensemble and fit to correct the prediction errors made by prior models. This is a type of ensemble machine learning model referred to as boosting.\n",
        "\n",
        "Models are fit using any arbitrary differentiable loss function and gradient descent optimization algorithm. This gives the technique its name, “gradient boosting,” as the loss gradient is minimized as the model is fit, much like a neural network.\n",
        "\n",
        "Extreme Gradient Boosting, or XGBoost for short is an efficient open-source implementation of the gradient boosting algorithm. As such, XGBoost is an algorithm, an open-source project, and a Python library.\n",
        "\n",
        "It was initially developed by Tianqi Chen and was described by Chen and Carlos Guestrin in their 2016 paper titled “XGBoost: A Scalable Tree Boosting System.”\n",
        "\n",
        "It is designed to be both computationally efficient (e.g. fast to execute) and highly effective, perhaps more effective than other open-source implementations.\n",
        "\n",
        "https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/\n",
        "\n",
        "XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems. The evidence is that it is the go-to algorithm for competition winners on the Kaggle competitive data science platform.\n",
        "\n",
        "\n",
        "With link you will discover how you can estimate the importance of features for a predictive modeling problem using the XGBoost library in Python.\n",
        "\n",
        "https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/\n",
        "\n",
        "XGB has a lot of usefull functions you can descover them here\n",
        "\n",
        "https://xgboost.readthedocs.io/en/latest/parameter.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_YB5_gpV5nR"
      },
      "source": [
        "# Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi9MFzV4V-JH"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import asarray\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrfwF4oJnytU"
      },
      "source": [
        "# Create data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRN_tAB3nx67",
        "outputId": "f3edc2e9-afc7-4002-d5e1-f36204f215ed"
      },
      "source": [
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
        "\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 20) (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2V594Ndn_-E"
      },
      "source": [
        "# XGBoost Ensemble for Regression\n",
        "\n",
        "### Model Creation & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dba3oFO6U0qX",
        "outputId": "56ff1664-cd82-4070-cd1c-a4feefe4e05d"
      },
      "source": [
        "# Define the model\n",
        "model = XGBClassifier()\n",
        "\n",
        "# KFold                   : is a cross-validator that divides the dataset into k folds.\n",
        "# Stratified              : is to ensure that each fold of dataset has the same proportion of observations with a given label.\n",
        "# StratifiedKFold         : is the improved version of KFold.\n",
        "# RepeatedStratifiedKFold : Repeats Stratified K-Fold n times with different randomization in each repetition.\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# Evaluate the model\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.899 (0.029)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OND35FZld4kU"
      },
      "source": [
        "### Classification Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sywE1HtedW4n",
        "outputId": "80a685b4-75c1-41e2-cd10-a0457903e80a"
      },
      "source": [
        "# fit the model on the whole dataset\n",
        "model.fit(X, y)\n",
        "\n",
        "# make a single prediction\n",
        "row = [0.2929949,-4.21223056,-1.288332,-2.17849815,-0.64527665,2.58097719,0.28422388,-7.1827928,-1.91211104,2.73729512,0.81395695,3.96973717,-2.66939799,3.34692332,4.19791821,0.99990998,-0.30201875,-4.43170633,-2.82646737,0.44916808]\n",
        "row = asarray([row])\n",
        "\n",
        "yhat = model.predict(row)\n",
        "print('Predicted Class: %d' % yhat[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[09:26:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Predicted Class: -105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6L0H2Jol4ct"
      },
      "source": [
        "# XGBoost Ensemble for Regression\n",
        "\n",
        "### Create data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c703Q8M7lwM0",
        "outputId": "3063875b-ff8e-43f3-be6a-20b501a5b64f"
      },
      "source": [
        "# define dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
        "\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 20) (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5225pP6onvC"
      },
      "source": [
        "### Model Creation and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Riw4FcA1l-2C",
        "outputId": "b8ba529c-e597-4f72-a39a-4e7344407af9"
      },
      "source": [
        "# define the model\n",
        "model = XGBRegressor()\n",
        "\n",
        "# evaluate the model\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\n",
        "# report performance\n",
        "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE: -62.762 (3.219)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRAFic9-mTtu"
      },
      "source": [
        "###  Regression Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-nm-MbFmVd1",
        "outputId": "c41a59d6-9d24-438f-f6eb-7d57ae2e41a6"
      },
      "source": [
        "# fit the model on the whole dataset\n",
        "model.fit(X, y)\n",
        "\n",
        "# make a single prediction\n",
        "row = [0.20543991,-0.97049844,-0.81403429,-0.23842689,-0.60704084,-0.48541492,0.53113006,2.01834338,-0.90745243,-1.85859731,-1.02334791,-0.6877744,0.60984819,-0.70630121,-1.29161497,1.32385441,1.42150747,1.26567231,2.56569098,-0.11154792]\n",
        "row = asarray([row])\n",
        "\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %d' % yhat[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[09:27:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 28\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}